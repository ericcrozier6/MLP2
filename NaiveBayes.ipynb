{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fbee7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "import math\n",
    "import statistics\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70cf3941",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load sparse matrix\n",
    "sparse_matrix = scipy.sparse.load_npz('training.npz')\n",
    "matrix = sparse_matrix.toarray()\n",
    "#print specific values\n",
    "\n",
    "#row 12 if the word 'of', so most news stories have it\n",
    "# print('matrix[1000][12]')\n",
    "# print(matrix[1000][12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a39e17b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 483], [2, 1107], [3, 1729], [4, 2372], [5, 2974], [6, 3604], [7, 4222], [8, 4836], [9, 5485], [10, 6113], [11, 6759], [12, 7398], [13, 8024], [14, 8645], [15, 9282], [16, 9933], [17, 10513], [18, 11106], [19, 11573], [20, 12000]]\n"
     ]
    }
   ],
   "source": [
    "#variables\n",
    "\n",
    "f = open(\"vocabulary.txt\", \"r\") \n",
    "\n",
    "#vocab length\n",
    "V = len(f.readlines())\n",
    "\n",
    "#beta and alpha values\n",
    "#beta = (1 / V)\n",
    "beta = .25\n",
    "alpha = 1 + beta\n",
    "\n",
    "#shape of matrix rows\n",
    "shape0 = matrix.shape[0]\n",
    "\n",
    "\n",
    "#turn far right col into its own matrix\n",
    "ys = np.zeros(shape=(matrix.shape[0],1))\n",
    "for i in range(shape0):\n",
    "    ys[i][0] = matrix[i][-1]\n",
    "#create a dictionary of the 20 labels and their occurences    \n",
    "unique, counts = np.unique(ys, return_counts=True)\n",
    "counts = dict(zip(unique, counts))\n",
    "\n",
    "\n",
    "columnIndex = -1 \n",
    "# Sort 2D numpy array by 2nd Column\n",
    "\n",
    "#recreate matrix but sorted by last col value\n",
    "sortedArr = matrix[matrix[:,columnIndex].argsort()]\n",
    "\n",
    "\n",
    "#create list of each of the 20 classifiers positions in the matrix\n",
    "amounts = []\n",
    "counter = 0\n",
    "index = 1\n",
    "for i in range(sortedArr.shape[0]):\n",
    "    if(index == sortedArr[i][-1]):\n",
    "        counter+=1\n",
    "    elif(i == sortedArr.shape[0] - 1):\n",
    "        amounts.append([index,counter])       \n",
    "    else:\n",
    "        amounts.append([index,counter])\n",
    "        index = sortedArr[i][-1]\n",
    "        counter += 1\n",
    "#amounts is the amount of each label in each doc (list of lists)        \n",
    "amounts.append([20,sortedArr.shape[0]])\n",
    "print(amounts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f194b2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e31f31eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLE function\n",
    "#takes in a new label labelY (0-20)\n",
    "#returns how many docs labeled the same as labelY / total docs\n",
    "def MLE(labelY):\n",
    "    #counts is a dictionary of label occurences\n",
    "    return counts[labelY + 1]/shape0\n",
    "    \n",
    "#MLE will have to change for testing set    \n",
    "\n",
    "\n",
    "#main purpose is to find the amount of uses of a wordI in labelY\n",
    "#and the total words in labelY\n",
    "def MAP(wordI,labelY):\n",
    "    totalWords = 0\n",
    "    totalY = 0\n",
    "    #if label is 1, its a different algorithm\n",
    "    if(labelY == 1):\n",
    "        #basically sum the sorted array from for the column wordI for the label 1\n",
    "        slice1 = amounts[0][1]\n",
    "        totalWords = sum(sum(sortedArr[:slice1,[wordI]]))\n",
    "        #sum all words for label 1\n",
    "        totalY = sum(sum(sortedArr[:slice1,1:-2]))\n",
    "    else: \n",
    "        slice1 = amounts[labelY -2][1]\n",
    "        slice2 = amounts[labelY - 1][1]\n",
    "        #sum all wordI for label\n",
    "        totalWords = sum(sum(sortedArr[slice1:slice2,[wordI]]))\n",
    "        #sum all words in the label, except first and last\n",
    "        totalY = sum(sum(sortedArr[slice1:slice2,1:-2]))\n",
    "    return (totalWords + beta) / (totalY + beta * V)\n",
    "\n",
    "'''\n",
    "#same as map\n",
    "#but returns the totalwords and totalY\n",
    "#done to save the results of out classifycations, but run them with different beta values\n",
    "def altMAP(wordI,labelY):\n",
    "    totalWords = 0\n",
    "    totalY = 0\n",
    "    #if label is 1, its a different algorithm\n",
    "    if(labelY == 1):\n",
    "        #basically sum the sorted array from for the column wordI for the label 1\n",
    "        slice1 = amounts[0][1]\n",
    "        totalWords = sum(sum(sortedArr[:slice1,[wordI]]))\n",
    "        #sum all words for label 1\n",
    "        totalY = sum(sum(sortedArr[:slice1,1:-2]))\n",
    "    else: \n",
    "        slice1 = amounts[labelY -2][1]\n",
    "        slice2 = amounts[labelY - 1][1]\n",
    "        #sum all wordI for label\n",
    "        totalWords = sum(sum(sortedArr[slice1:slice2,[wordI]]))\n",
    "        #sum all words in the label, except first and last\n",
    "        totalY = sum(sum(sortedArr[slice1:slice2,1:-2]))\n",
    "    return [wordI, labelY, totalWords, totalY]\n",
    "'''\n",
    "\n",
    "\n",
    "def classify(wordI):\n",
    "    wordCount = matrix[:, wordI].sum()\n",
    "    arr = []\n",
    "    #loop through all 20 labels        \n",
    "    for i in range(20):\n",
    "        #take the MLE and MAP for all labels\n",
    "        mp = math.log(MAP(wordI,i),2)\n",
    "        mle = MLE(i)\n",
    "        if(mle > 0):\n",
    "            mle = math.log(mle,2)\n",
    "        #add to list    \n",
    "        arr.append(mle + (wordCount * mp) )   \n",
    "    #return the most likely label     \n",
    "    idx = arr.index(max(arr)) \n",
    "    return idx + 1\n",
    "\n",
    "    \n",
    "\n",
    "dic = {}\n",
    "#helper function\n",
    "#classifies all words in a given doc\n",
    "def allWordsClassify(row):\n",
    "    \n",
    "    #take row and get all non zero elements \n",
    "    areRow = matrix[row-1,:]\n",
    "    indexes = np.nonzero(areRow)\n",
    "    pred = []\n",
    "    #loop through all words in doc\n",
    "    for i in indexes:\n",
    "        for j in i:\n",
    "            #if we have classified the word then dont bother classifying again \n",
    "            if j in dic:\n",
    "                pred.append(dic[j])\n",
    "            else:\n",
    "                cls = classify(j)\n",
    "                pred.append(cls)\n",
    "                dic[j] = cls\n",
    "        break    \n",
    "    #return most highest predicted label    \n",
    "    return mode(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd5606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "This section is to classify a word, and store it in our text file\n",
    "Once a word is classified and stored, we never need to classify it again\n",
    "since that is the major source of slowdown\n",
    "\n",
    "Note this code is very very slow. It takes around 8 hours to classify every word\n",
    "But once this is done, the documents can be classified instantly. \n",
    "\n",
    "'''\n",
    "#file to write to \n",
    "f = open(\"words.txt\", \"a\")\n",
    "\n",
    "ls = []\n",
    "start = 0\n",
    "end = shape0\n",
    "#loop through every word\n",
    "for i in range(len(lines),matrix.shape[1]-1):\n",
    "    #classify word, store it in text file for later\n",
    "    x = classify(i)\n",
    "    f.write(str(x) + '\\n')\n",
    "    print(str(x))\n",
    "    #ls.append(x)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "06b28e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61190\n",
      "61190\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This section opens the text file with our classified words\n",
    "It then takes all the words used in a given doc from the file \n",
    "Then it takes the mode (most used label), and adds it to a list \n",
    "\n",
    "'''\n",
    "lines = []\n",
    "#extract all classifications for words into a list\n",
    "with open('words.txt') as file:\n",
    "    lines = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "f = open(\"classes5.txt\", \"a\")   \n",
    "print(len(lines))\n",
    "print(matrix.shape[1])\n",
    "classified = []\n",
    "checking = []\n",
    "#loop through every doc, and all words in doc\n",
    "for i in range(matrix.shape[0] - 1):\n",
    "    for j in range(matrix.shape[1] - 1):\n",
    "        #if doc has that word\n",
    "        if(matrix[i][j] != 0):\n",
    "            #take the classification for word and store it\n",
    "            checking.append(lines[j])\n",
    "    #docs classification is most often classification from all words in doc        \n",
    "    classified.append(mode(checking))\n",
    "    #save data to file for later\n",
    "    f.write(str(mode(checking)) + '\\n')\n",
    "    \n",
    "    checking = []\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "285bdbe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount correct:\n",
      "8779\n",
      "Percentage: \n",
      "0.7316443036919743\n",
      "\n",
      "\n",
      " Confusion matrix:\n",
      "[[458   0   0  67 107  64   7  60  29  37  55 161 126 184  79 176  99  99\n",
      "  228 326]\n",
      " [  0 485   0   0   5  20   0   0   1   0   2   7  13   5  13   0   0   0\n",
      "    0   1]\n",
      " [  0   0 523   0   0   3   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0 528   0   0   4   1   0   1   1   0   9   1   0   1   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0 448   0   0   8   1   5   3   4  29   8   2   0   0   0\n",
      "    0   0]\n",
      " [  0  28   0   0   0 520   0   0   0   0   0   5  10   2   0   0   0   2\n",
      "    0   0]\n",
      " [  3  59  42   0   0   0 596   0   0  12   6  13  64  27  16   6   5   6\n",
      "    7   0]\n",
      " [  0   6   5   8   0   0   0 514   0   0   4   5  14   3   2   4   2   3\n",
      "    4   1]\n",
      " [  1   5  10   7   3   0   0   0 603   0   0  12  19   8   6   1   6   3\n",
      "    8   3]\n",
      " [  5  10  16   9  20   6   0   0   0 547   0   0  28  34  19  12  12  12\n",
      "   30   5]\n",
      " [  0   1   0   0   0   2   0   0   0   0 553   0   0   0   0   1   0   0\n",
      "    3   0]\n",
      " [  1   3   0   1   0   0   0   0   0   0   0 375   0   0   0   1   2   0\n",
      "    2   0]\n",
      " [  0   0   0   0   2   1   1   0   0   0   0   0 275   0   0   0   1   0\n",
      "    0   0]\n",
      " [  1   0   1   1   0   0   1   0   0   0   0   0   0 305   0   0   0   0\n",
      "    1   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   0   0   0 500   0   0   0\n",
      "    0   0]\n",
      " [  3  14   7   5   0   5   2   4   3   3   3  17   0   0   0 449   0   0\n",
      "   28  46]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   4   0   0   0   0 452   0\n",
      "    0   2]\n",
      " [ 11  12  18  17  17   9   7  27  12  23  18  35  39  44   0   0   0 468\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "  156   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0\n",
      "    0  43]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Here we use the list created above with the label guess for each doc\n",
    "Then get the actual label for each doc\n",
    "Zip them together, check if we are correct\n",
    "And return the % correct\n",
    "\n",
    "'''\n",
    "classes = []\n",
    "#use data gathered above\n",
    "with open('classes.txt') as file:\n",
    "    classes = [line.rstrip('\\n') for line in file]\n",
    "    \n",
    "start = 1\n",
    "end = matrix.shape[0]\n",
    "count = 0\n",
    "actually = []\n",
    "confusion = np.zeros((20, 20) , dtype=np.int64)\n",
    "#loop through all docs and get actual label\n",
    "for i in range(start,end):\n",
    "    actually.append(matrix[i-1][-1])\n",
    "    \n",
    "paPairs = zip(classes,actually)\n",
    "#loop through classifications and see if we are right\n",
    "for prediction, actual in paPairs:\n",
    "    if str(prediction) == str(actual):\n",
    "        count+=1 \n",
    "    #record value in confusion matrix    \n",
    "    confusion[int(prediction) - 1][int(actual) - 1] += 1\n",
    "    \n",
    "    \n",
    "print(\"Amount correct:\")     \n",
    "print(count)        \n",
    "print(\"Percentage: \")\n",
    "print(count / (end - start))\n",
    "\n",
    "print(\"\\n\\nConfusion matrix:\")\n",
    "print(confusion)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165643c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
