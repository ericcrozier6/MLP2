{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02118acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "993ee65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#read csv\n",
    "load_test_set = 'testing.csv'\n",
    "df = pd.read_csv(load_test_set, header=None)\n",
    "\n",
    "#convert dataframe to numpy array\n",
    "#then create a sparse matrix from that\n",
    "matrix = scipy.sparse.csr_matrix(df.to_numpy())\n",
    "\n",
    "#print array just to see\n",
    "print(matrix.toarray())\n",
    "\n",
    "#save sparse array as npz file \n",
    "save_file = 'testing.npz'\n",
    "scipy.sparse.save_npz(save_file, matrix)\n",
    "print('done')\n",
    "# we only needed to run this code twice, once for the testing data and once for the training data\n",
    "#'''\n",
    "\n",
    "#load in the training data set from the previously saved .npz\n",
    "Xtrain = scipy.sparse.load_npz('training.npz')\n",
    "\n",
    "#save the class values here for later as a flat array\n",
    "Ytrain = Xtrain[:,61189].toarray().flatten()\n",
    "\n",
    "#extract just the attributes and the ids from the data\n",
    "Xtrain = Xtrain[:,:61189]\n",
    "\n",
    "#save the IDs for later (might be obsolete)\n",
    "trainDataSetIDs = Xtrain[:,0].toarray().flatten()\n",
    "\n",
    "#overwrite the IDs as ones this accounts for the extra weight we will be calcualting\n",
    "Xtrain[:,0] = 1\n",
    "\n",
    "#calculate and save the column sums of this matrix for normalization later\n",
    "#these values are the sums of each attribute across all the examples\n",
    "colSumsTrain = Xtrain[:,:61189].sum(axis=0).astype(float)\n",
    "\n",
    "#take the recipricol of the column sums where they are not 0\n",
    "recipricol = np.reciprocal(colSumsTrain, where=colSumsTrain!=0).A1\n",
    "\n",
    "#build a diagonal matrix from the previously calculated recipricols\n",
    "diag = scipy.sparse.diags(recipricol)\n",
    "\n",
    "#matrix multiply the diagonal matrix agains the train matrix,\n",
    "#this normalizes the values of each attribute so they sum to 1,\n",
    "#by elementwise dividing each column by the sum of that column\n",
    "Xtrain = Xtrain @ diag\n",
    "\n",
    "#build the delta matrix for the training data,\n",
    "#this is a matrix, where each row is a class and each column an example\n",
    "#each example has a one in the row for the class it belongs to,\n",
    "Deltatrain = np.zeros((20,12000), dtype=int)\n",
    "count = 0\n",
    "for i in Ytrain:\n",
    "    Deltatrain[i-1][count] = 1\n",
    "    count+=1\n",
    "Deltatrain = scipy.sparse.csr_matrix(Deltatrain)\n",
    "\n",
    "#load in the test data from its .npz\n",
    "Xtest = scipy.sparse.load_npz('testing.npz')\n",
    "\n",
    "#extract the IDs\n",
    "testDataSetIDs = Xtest[:,0].toarray().flatten()\n",
    "\n",
    "#overwrite the IDs with ones to account for our extra weight\n",
    "Xtest[:,0] = 1\n",
    "\n",
    "#normalize the matrix using the values from the training data\n",
    "Xtest = Xtest @ diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cab2b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes a matrix of weights and a given sparse matrix of attributes\n",
    "#the matricies must have the same number of columns\n",
    "#this function calcualtes the probabilities for each class each example belongs to that class\n",
    "#using a provided weight matrix\n",
    "def getProbabilitys(W, X):\n",
    "    #multiply the weights by the transpose of the attributes,\n",
    "    #this gives us a matrix of probabilites where the rows are the classes and the columns examples\n",
    "    unNormalized = W @ X.transpose()\n",
    "    \n",
    "    #we take the exponential of this matrix, if we had not normalized before we would overflow\n",
    "    probabilitys = np.exp(unNormalized)\n",
    "    \n",
    "    #we normalize again to prevent overflow in subsequent iterations \n",
    "    #and to make each examples probabilities sum to one\n",
    "    probabilitys = normalize(probabilitys, norm = 'l1', axis = 0)\n",
    "    return probabilitys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25724b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function classifies a given data set with a given eta, and lambda value\n",
    "# over a specific number of training iterations, the given eta and lambda values must corespond\n",
    "# to a classFunction already saved in the working folder, this is the function output by the train method\n",
    "#dataSet = the data set you would like to classify either 'training' or 'testing'\n",
    "#eta = the eta value for the trainer, a learing rate, effects how large of jumps the trainer makes\n",
    "#lambda = a penalty rate, penalizes large values of weights that may be outliers\n",
    "#iterations = the number of times the trainier will train on the training set before making its classifications\n",
    "#NOTE: we have not implimented a stopping criteria beyond this value at the time of this writing,\n",
    "#though on can be implimented using the error value calculated in the updateWeights function\n",
    "def classify(dataSet, eta, lmdb, iterations):\n",
    "    #read in the classFunction from the working folder that coresponds to the given parameters\n",
    "    classFunctionFileName = 'class_function_eta' + str(eta) + '_lmdb' + str(lmdb) + '_iter' + str(iterations)\n",
    "    classFunctionDF = pd.read_csv(classFunctionFileName+'.csv', header=None)\n",
    "    \n",
    "    #this will be our weight matrix we use to classify the data set\n",
    "    W = classFunctionDF.to_numpy()\n",
    "    \n",
    "    #grab the data set we need based on the parameters\n",
    "    #we also grab the ids for the output\n",
    "    if dataSet == 'training':\n",
    "        X = Xtrain\n",
    "        dataSetIDs = trainDataSetIDs\n",
    "    else:\n",
    "        X = Xtest\n",
    "        dataSetIDs = testDataSetIDs\n",
    "    \n",
    "    #calculate the probabilities for the data using the chosen weight function\n",
    "    probabilities = getProbabilitys(W, X)\n",
    "    \n",
    "    #choose our predictions as the highest probability for each example\n",
    "    # note that argmax returns the index of the highest value and so is 0 indexed\n",
    "    # our class ids are 1 index so we must shift our results by 1\n",
    "    predictions = np.argmax(probabilities, axis=0)+1\n",
    "    \n",
    "    #stack the predictions and their example IDs and convert to dataframe for output\n",
    "    output = pd.DataFrame(np.column_stack((dataSetIDs, predictions)))\n",
    "    \n",
    "    #generate descriptive file name from parameters\n",
    "    outputFileName=dataSet.upper()+'predictions_eta'+str(eta)+'_lmdb'+str(lmdb)+'_iter'+str(iterations)+'.csv'\n",
    "    \n",
    "    #save the output file of our predictions in the appropriate format\n",
    "    output.to_csv(outputFileName, header=['id', 'class'], index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83ba364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes a weight matrix and updates it,\n",
    "# using our hyperparameters, the class values, and our attribute matrix\n",
    "#W is the weight matrix to be updated\n",
    "#eta is our step size hyperparameter\n",
    "#delta is the delta matrix we generated from the training data\n",
    "#probabilitys is a matrix of probabilites assumed to be genereated from the given W using getProbabilitys\n",
    "#X is the current data sets attribute matrix\n",
    "#lmdb is our lambda penalty hyperparameter\n",
    "def updateWeights(W, eta, delta, probabilitys, X, lmdb):\n",
    "    #delta - probabilitys gives us our error, which could be used as a stopping parameter as mentioned above\n",
    "    #this value is multiplied by the attribute matrix to give us the error accross the attributes\n",
    "    #this is then penalized by the lmdb value across out weights\n",
    "    #multiplied by our step size and added to our weights\n",
    "    #this gives us new weights that are moved in the direction of our known true class values\n",
    "    WNew = W + eta*( ((delta - probabilitys) @ X) - (lmdb*W) )\n",
    "    \n",
    "    #return the array of our new weights\n",
    "    return np.array(WNew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54091643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function trains a weight matrix for the given data set, hyperparameters and iterations\n",
    "#W is a starting weight function, usually generated randomly\n",
    "#eta and lmdb our our step size and penalty hyperparameters respectivily\n",
    "#delta is our dleta matrix generated from the training data true class values\n",
    "#iterations is the number of iterations we would like to train on\n",
    "#note: to impliment an error stoping criteria we would do it here, updateWeights would return the error\n",
    "# value as well and that would be the stoping value for the loop.\n",
    "def train(W, X, eta, delta, lmdb, iterations):\n",
    "    WNew = updateWeights(W, eta, delta, getProbabilitys(W, X), X, lmdb)\n",
    "    for i in range(iterations):\n",
    "        WNew = updateWeights(WNew, eta, delta, getProbabilitys(WNew, X), X, lmdb)\n",
    "    return WNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6975828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# train over for parameters eta in range [0.01, 0.007, 0.005, 0.002, 0.001]\\n# save resulting classFunctions, and predictions over the test set to files\\n#paramRange = [0.01, 0.005, 0.001]\\nparamRange = [0.009, 0.008, 0.007, 0.006]\\nfor eta in paramRange:\\n    #for lmdb in paramRange:\\n    lmdb=0.01\\n    W = train(WStart, Xtrain, eta, Deltatrain, lmdb, 10000)\\n    np.savetxt('class_function_eta'+str(eta)+'_lmdb'+str(lmdb)+'_iter10000.csv', W, delimiter=',')\\n    classify('testing', eta, lmdb, 10000)\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Utilization of this Code\n",
    "# the code below shows an example of training 5 different weight functions for different eta values\n",
    "# peices of it can be uncommented to allow for traning over lambda values as well\n",
    "# each weight function is saved for use by the classify function after it is generated\n",
    "\n",
    "#The general use case\n",
    "#1 call train using train(WStart, Xtrain, eta, Deltatrain, lmdb, iterations),\n",
    "# where eta, lmdb, and iterations are replaced by your chosen values\n",
    "#2 make sure to save the return value of train to a variable, I usually use W\n",
    "#3 save that W file to the working directory using np.savetxt and the string format detailed below\n",
    "# ie. 'class_function_eta'+str(eta)+'_lmdb'+str(lmdb)+'_iter'+str(iterations)'.csv'\n",
    "# your saved weight function and delimiter=','\n",
    "#4 call classify with the data set you would like to classify, in this case either 'testing' or 'training'\n",
    "\n",
    "#This is so split up so that you may run train without saving it to a file \n",
    "# or run classify on weight functions already in memory without having to train them yourself\n",
    "\n",
    "#The function in the next cell will run through a full train and classify for the given hyperparameters\n",
    "\n",
    "'''\n",
    "# train over for parameters eta in range [0.01, 0.007, 0.005, 0.002, 0.001]\n",
    "# save resulting classFunctions, and predictions over the test set to files\n",
    "#paramRange = [0.01, 0.005, 0.001]\n",
    "paramRange = [0.009, 0.008, 0.007, 0.006]\n",
    "for eta in paramRange:\n",
    "    #for lmdb in paramRange:\n",
    "    lmdb=0.01\n",
    "    W = train(WStart, Xtrain, eta, Deltatrain, lmdb, 10000)\n",
    "    np.savetxt('class_function_eta'+str(eta)+'_lmdb'+str(lmdb)+'_iter10000.csv', W, delimiter=',')\n",
    "    classify('testing', eta, lmdb, 10000)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b05ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#single run through of the logistic regression process\n",
    "#dataSet = the data set you would like to classify, for this example must be 'training' or 'testing'\n",
    "#eta = your step size\n",
    "#lmdb = your penalty term\n",
    "#iterations = the number of iterations you would like to train on\n",
    "def logisticRegression(dataSet, eta, lmdb, iterations):\n",
    "    WStart = np.random.rand(20,61189)\n",
    "    W = train(WStart, Xtrain, eta, Deltatrain, lmdb, iterations)\n",
    "    np.savetxt('class_function_eta'+str(eta)+'_lmdb'+str(lmdb)+'_iter'+str(iterations)+'.csv', W, delimiter=',')\n",
    "    classify(dataSet, eta, lmdb, 10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
